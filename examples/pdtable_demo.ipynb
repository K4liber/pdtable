{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo of `pdtable`\n",
    "\n",
    "The `pdtable` package allows working with StarTable tables as pandas dataframes while consistently keeping track of StarTable-specific metadata such as table destinations and column units.\n",
    "\n",
    "This is implemented by providing two interfaces to the same backing object:\n",
    "- `PandasTable` is derived from `pandas.DataFrame`. It carries the data and is the interface of choice for interacting with the data using the pandas API, with all the convenience that this entails.  It also carries StarTable metadata, but in a less accessible way.\n",
    "- `Table` is a stateless fa√ßade for a backing `PandasTable` object. `Table` is the interface of choice for convenient access to StarTable-specific metadata such as column units, and table destinations and origin. Some data manipulation is also possible in the `Table` interface, though this functionality is limited.\n",
    "\n",
    "This notebook provides a demo of how this works. See the `pdtable` docstring for a discussion of the implementation.\n",
    "\n",
    "## Idea\n",
    "\n",
    "The central idea is that as much as possible of the table information is stored as a pandas dataframe,\n",
    "and that the remaining information is stored as a `TableData` object attached to the dataframe as registered metadata.\n",
    "Further, access to the full table datastructure is provided through a facade object (of class `Table`). `Table` objects\n",
    "have no state (except the underlying decorated dataframe) and are intended to be created when needed and discarded afterwards:\n",
    "\n",
    "```\n",
    "dft = make_pdtable(...)\n",
    "unit_height = Table(dft).height.unit\n",
    "```\n",
    "\n",
    "Advantages of this approach are that:\n",
    "\n",
    "1. Code can be written for (and tested with) pandas dataframes and still operate on `PandasTable` objects.\n",
    "   This frees client code from necessarily being coupled to the startable project.\n",
    "   A first layer of client code can read and manipulate StarTable data, and then pass it on as a (`PandasTable`-flavoured) `pandas.DataFrame` to a further layer having no knowledge of `pdtable`.\n",
    "2. The access methods of pandas `DataFrames`, `Series`, etc. are available for use by consumer code via the `PandasTable` interface. This both saves the work\n",
    "   of re-implementing similar access methods on a StarTable-specific object, and likely allows better performance and documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from textwrap import dedent\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added repo root dir to sys.path: C:\\Users\\jeaco\\repos\\pdtable\n"
     ]
    }
   ],
   "source": [
    "# Workaround for this demo in notebook form:\n",
    "# Notebooks don't have repo root in sys.path\n",
    "# Better solutions are welcome...\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "if Path.cwd().name == \"examples\":\n",
    "    # Maybe we're in a notebook in the /examples folder\n",
    "    repo_root_dir = str(Path.cwd().parent)\n",
    "    if repo_root_dir not in sys.path:\n",
    "        sys.path.append(repo_root_dir)\n",
    "        print(f\"Added repo root dir to sys.path: {repo_root_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The `Table` aspect\n",
    "\n",
    "The table aspect presents the table with a focus on StarTable metadata.\n",
    "Data manipulation functions exist, and more are easily added -- but focus is on metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdtable import pandastable\n",
    "from pdtable.proxy import Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "**mytable\n",
       "all\n",
       "   a [m] b [text]\n",
       "0      0  the foo\n",
       "1      1  the foo\n",
       "2      2  the foo\n",
       "3      3  the foo\n",
       "4      4  the foo"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = Table(name=\"mytable\")\n",
    "\n",
    "# Add columns explicitly...\n",
    "t.add_column(\"a\", range(5), \"km\")\n",
    "\n",
    "# ...or via item access\n",
    "t[\"b\"] = [\"the foo\"] * 5\n",
    "\n",
    "# Modify column metadata through column proxy objets:\n",
    "t[\"a\"].unit = \"m\"\n",
    "\n",
    "# Table renders as annotated dataframe\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'b']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column(name='b', unit='text', values=<PandasArray>\n",
       "['the foo', 'the foo', 'the foo', 'the foo', 'the foo']\n",
       "Length: 5, dtype: object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Each column has associated metadata object that can be manipulated:\n",
    "t[\"b\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['m', 'text']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'**mytable\\nall\\n   a [m] b [text]\\n0      0  the foo\\n1      1  the foo\\n2      2  the foo\\n3      3  the foo\\n4      4  the foo'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TableMetadata(name='mytable', destinations={'all'}, operation='Created', parents=[], origin='')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "**table2\n",
       "all\n",
       "   c [m]  d [kg]\n",
       "0      1       4\n",
       "1      2       5\n",
       "2      3       6"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating table from pandas dataframe:\n",
    "t2 = Table(pd.DataFrame({\"c\": [1, 2, 3], \"d\": [4, 5, 6]}), name=\"table2\", units=[\"m\", \"kg\"])\n",
    "t2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The `PandasTable` / `pd.DataFrame` aspect\n",
    "\n",
    "Both the table contents and metadata displayed and manipulated throught the `Table`-class is stored as a `PandasTable` object, which is a normal pandas dataframe with two modifications:\n",
    "\n",
    "* It has a `_table_data` field registered as a metadata field, so that pandas will include it in copy operations, etc.\n",
    "* It will preserve column and table metadata for some pandas operations, and fall back to returning a normal dataframe if this is not possible/implemented.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "**mytable\n",
       "all\n",
       "   a [m] b [text]\n",
       "0      0  the foo\n",
       "1      1  the foo\n",
       "2      2  the foo\n",
       "3      3  the foo\n",
       "4      4  the foo"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# After getting a reference to the dataframe backing t, it is safe to delete t:\n",
    "df = t.df\n",
    "del t\n",
    "Table(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interacting with table metadata form  without the Table proxy is slightly verbose\n",
    "df2 = pandastable.make_pdtable(\n",
    "    pd.DataFrame({\"c\": [1, 2, 3], \"d\": [4, 5, 6]}), name=\"table2\", units=[\"m\", \"kg\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "**mytable\n",
       "all\n",
       "   a [m] b [text]  c [m]  d [kg]\n",
       "0      0  the foo    1.0     4.0\n",
       "1      1  the foo    2.0     5.0\n",
       "2      2  the foo    3.0     6.0\n",
       "3      3  the foo    NaN     NaN\n",
       "4      4  the foo    NaN     NaN"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example: combining columns from multiple tables\n",
    "df_combinded = pd.concat([df, df2], sort=False, axis=1)\n",
    "Table(df_combinded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fundamental issues with the facade approach\n",
    "\n",
    "The key problem with not integrating more tightly with the dataframe datastructure is that some operations will not trigger `__finalize__` and thus have to be deciphered based on the dataframe and metadata states.\n",
    "\n",
    "One example of this is column names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata columns before update triggered by access:\n",
      "{'a': ColumnMetadata(unit='m', display_unit=None, display_format=None), 'b': ColumnMetadata(unit='text', display_unit=None, display_format=None)}\n",
      "\n",
      "**mytable\n",
      "all\n",
      "   a_new [-] b_new [text]\n",
      "0          0      the foo\n",
      "1          1      the foo\n",
      "2          2      the foo\n",
      "3          3      the foo\n",
      "4          4      the foo\n",
      "Metadata columns after update:\n",
      "{'a_new': ColumnMetadata(unit='-', display_unit=None, display_format=None), 'b_new': ColumnMetadata(unit='text', display_unit=None, display_format=None)}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_renamed = df.copy()\n",
    "df_renamed.columns = [\"a_new\", \"b_new\"]\n",
    "print(f\"Metadata columns before update triggered by access:\\n{df_renamed._table_data.columns}\\n\")\n",
    "print(Table(df_renamed))\n",
    "print(f\"Metadata columns after update:\\n{df_renamed._table_data.columns}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## I/O\n",
    "StarTable data can be read from, and written to, CSV files and Excel workbooks. In its simplest usage, the pdtable.io API can be illustrated as:\n",
    "![pdtable.io simplified API](..\\docs\\diagrams\\img\\io_simple\\io_simple.svg)\n",
    "We will return to a more detailed diagram also illustrating JSON support. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading\n",
    "The functions `read_excel()` and `read_csv()` do what it says on the tin.\n",
    "`read_csv()` can read from files as well as text streams.\n",
    "\n",
    "These generate tuples of (`block_type`, `block`) where\n",
    "* `block` is a StarTable block\n",
    "* `block_type` is a `BlockType` enum that indicates which type of StarTable block `block` is. Possible values: `DIRECTIVE`, `TABLE`, `TEMPLATE_ROW`, `METADATA`, and `BLANK`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make some CSV StarTable data, put it in a text stream, and read that stream. (Could just as well have been a CSV file.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from io import StringIO\n",
    "from pdtable import read_csv\n",
    "\n",
    "csv_data = StringIO(\n",
    "    dedent(\n",
    "        \"\"\"\\\n",
    "    author: ;XYODA     ;\n",
    "    purpose:;Save the galaxy\n",
    "\n",
    "    ***gunk\n",
    "    grok\n",
    "    jiggyjag\n",
    "\n",
    "    **places;\n",
    "    all\n",
    "    place;distance;ETA;is_hot\n",
    "    text;km;datetime;onoff\n",
    "    home;0.0;2020-08-04 08:00:00;1\n",
    "    work;1.0;2020-08-04 09:00:00;0\n",
    "    beach;2.0;2020-08-04 17:00:00;1\n",
    "\n",
    "    **farm_animals;;;\n",
    "    your_farm my_farm other_farm;;;\n",
    "    species;n_legs;avg_weight;\n",
    "    text;-;kg;\n",
    "    chicken;2;2;\n",
    "    pig;4;89;\n",
    "    cow;4;200;\n",
    "    unicorn;4;NaN;\n",
    "    \"\"\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Read the stream. Syntax is the same if reading CSV file.\n",
    "# Reader function returns a generator; unroll it in a list for convenience.\n",
    "block_list = list(read_csv(csv_data))\n",
    "assert len(block_list) == 6  # includes blanks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reader generates tuples of `(BlockType, block)`. Note that blank/comment lines are read but not parsed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BlockType.METADATA <class 'pdtable.ancillary_blocks.MetadataBlock'>\n",
      "BlockType.DIRECTIVE <class 'pdtable.ancillary_blocks.Directive'>\n",
      "BlockType.BLANK <class 'list'>\n",
      "BlockType.TABLE <class 'pdtable.proxy.Table'>\n",
      "BlockType.BLANK <class 'list'>\n",
      "BlockType.TABLE <class 'pdtable.proxy.Table'>\n"
     ]
    }
   ],
   "source": [
    "for bt, b in block_list:\n",
    "    print(bt, type(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's one of the tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**places\n",
      "all\n",
      "  place [text]  distance [km]      ETA [datetime]  is_hot [onoff]\n",
      "0         home            0.0 2020-08-04 08:00:00            True\n",
      "1         work            1.0 2020-08-04 09:00:00           False\n",
      "2        beach            2.0 2020-08-04 17:00:00            True\n"
     ]
    }
   ],
   "source": [
    "t = block_list[3][1]\n",
    "assert t.name == 'places'\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can pick the tables (and leave out blocks of other types) like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdtable import BlockType\n",
    "tables = [b for bt, b in block_list if bt == BlockType.TABLE]\n",
    "assert len(tables) == 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtered reading\n",
    "Blocks can be filtered prior to parsing, by passing to a callable as `filter` argument to the reader functions. This can\n",
    "reduce reading time substantially when reading only a few tables from an otherwise large file\n",
    "or stream. \n",
    "\n",
    "The callable must accept two arguments: block type, and block name. Only those blocks for which the filter callable returns `True` are fully parsed. Other blocks\n",
    "are parsed only superficially i.e. only the block's top-left cell, which is just\n",
    "enough to recognize block type and name to pass to the filter callable, thus avoiding the much more\n",
    "expensive task of parsing the entire block, e.g. the values in all columns and rows of a large\n",
    "table.\n",
    "\n",
    "Let's design a filter that only accepts tables whose name contains the word `'animal'`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_table_about_animals(block_type: BlockType, block_name: str) -> bool:\n",
    "    return block_type == BlockType.TABLE and \"animal\" in block_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see what happens when we use this filter when re-reading the same CSV text stream as before. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "**farm_animals\n",
       "other_farm, my_farm, your_farm\n",
       "  species [text]  n_legs [-]  avg_weight [kg]\n",
       "0        chicken         2.0              2.0\n",
       "1            pig         4.0             89.0\n",
       "2            cow         4.0            200.0\n",
       "3        unicorn         4.0              NaN"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_data.seek(0)\n",
    "block_list = list(read_csv(csv_data, filter=is_table_about_animals))\n",
    "\n",
    "assert len(block_list) == 1\n",
    "block_list[0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Non-table blocks were ignored, and so were table blocks that weren't animal-related."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing\n",
    "Use the aptly named `write_csv()` and `write_excel()`. Note that `write_csv()` can write to files as well as to text streams. \n",
    "\n",
    "Let's write the tables we read earlier to a text stream. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**places;\n",
      "all\n",
      "place;distance;ETA;is_hot\n",
      "text;km;datetime;onoff\n",
      "home;0.0;2020-08-04 08:00:00;1\n",
      "work;1.0;2020-08-04 09:00:00;0\n",
      "beach;2.0;2020-08-04 17:00:00;1\n",
      "\n",
      "**farm_animals;\n",
      "other_farm my_farm your_farm\n",
      "species;n_legs;avg_weight\n",
      "text;-;kg\n",
      "chicken;2.0;2.0\n",
      "pig;4.0;89.0\n",
      "cow;4.0;200.0\n",
      "unicorn;4.0;-\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pdtable import write_csv\n",
    "\n",
    "with StringIO() as s:\n",
    "    write_csv(tables, s)\n",
    "    \n",
    "    print(s.getvalue())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JSON support\n",
    "StarTable data can be converted to and from a `JsonData` object i.e. a JSON-ready data structure of nested dicts (\"objects\"), lists (\"arrays\"), and JSON-native values. This `JsonData` can then be serialized and deserialized directly using the standard library's `json.dump()` and `json.load()`. \n",
    "\n",
    "A `JsonData` representation is currently only defined for table blocks. \n",
    "\n",
    "The pdtable.io API including JSON support can be illustrated as a more detailed version of the diagram shown earlier:\n",
    "\n",
    "![pdtable.io API](..\\docs\\diagrams\\img\\io_detailed\\io_detailed.svg)\n",
    "\n",
    "In addition, two successive intermediate data structures are visible in this diagram: \n",
    "\n",
    "- \"Raw cell grid\" is simply a sequence of sequence of values (usually a list of lists or list of tuples) interpreted as rows of cells, with cell contents as read in their raw form from CSV or Excel. \n",
    "- `JsonDataPrecursor` is essentially the same thing as `JsonData`, with the exception that it may contain values that are not directly consumable by `json.dump()`. These values must be converted further if the aim is a `JsonData`; for example Numpy arrays to lists, `datetime.datetime` to a string representation thereof, etc. \n",
    "\n",
    "Let's re-read the CSV stream we created earlier, but specifying the argument `to='jsondata'` such that tables should be parsed to JSON data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'places',\n",
       " 'columns': {'place': ['home', 'work', 'beach'],\n",
       "  'distance': [0.0, 1.0, 2.0],\n",
       "  'ETA': ['2020-08-04 08:00:00', '2020-08-04 09:00:00', '2020-08-04 17:00:00'],\n",
       "  'is_hot': [True, False, True]},\n",
       " 'units': ['text', 'km', 'datetime', 'onoff'],\n",
       " 'destinations': {'all': None},\n",
       " 'origin': '<_io.StringIO object at 0x00000163263ED040>'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_data.seek(0)\n",
    "block_gen = read_csv(csv_data, to='jsondata')\n",
    "json_ready_tables = [b for bt, b in block_gen if bt == BlockType.TABLE]\n",
    "\n",
    "json_ready_tables[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data structure is now easily serialized as JSON:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"name\": \"places\", \"columns\": {\"place\": [\"home\", \"work\", \"beach\"], \"distance\": [0.0, 1.0, 2.0], \"ETA\": [\"2020-08-04 08:00:00\", \"2020-08-04 09:00:00\", \"2020-08-04 17:00:00\"], \"is_hot\": [true, false, true]}, \"units\": [\"text\", \"km\", \"datetime\", \"onoff\"], \"destinations\": {\"all\": null}, \"origin\": \"<_io.StringIO object at 0x00000163263ED040>\"}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "table_json = json.dumps(json_ready_tables[0])\n",
    "print(table_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are also utilities to convert back and forth between `JsonData` and `Table`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "**places\n",
       "all\n",
       "  place [text]  distance [km]      ETA [datetime]  is_hot [onoff]\n",
       "0         home            0.0 2020-08-04 08:00:00            True\n",
       "1         work            1.0 2020-08-04 09:00:00           False\n",
       "2        beach            2.0 2020-08-04 17:00:00            True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pdtable.io import json_data_to_table, table_to_json_data\n",
    "t = json_data_to_table(json_ready_tables[0])\n",
    "t  # It's now a Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'places',\n",
       " 'origin': '<_io.StringIO object at 0x00000163263ED040>',\n",
       " 'destinations': {'all': None},\n",
       " 'units': ['text', 'km', 'datetime', 'onoff'],\n",
       " 'columns': {'place': ['home', 'work', 'beach'],\n",
       "  'distance': [0.0, 1.0, 2.0],\n",
       "  'ETA': ['2020-08-04 08:00:00', '2020-08-04 09:00:00', '2020-08-04 17:00:00'],\n",
       "  'is_hot': [True, False, True]}}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_to_json_data(t)  # Now it's back to JsonData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling directives\n",
    "StarTable Directive blocks are intended to be interpreted and handled at read time, and then discarded. The client code (i.e. you) is responsible for doing this. Handling directives is done outside of the `pdtable` framework. This would typically done by a function that consumes a `BlockGenerator` (the output of the reader functions), processes the directive blocks encountered therein, and in turn yields a processed `BlockGenerator`, as in:\n",
    "\n",
    "```\n",
    "def handle_some_directive(bg: BlockGenerator, *args, **kwargs) -> BlockGenerator:\n",
    "    ...\n",
    "```\n",
    "\n",
    "Usage would then be:\n",
    "```\n",
    "block_gen = handle_some_directive(read_csv('foo.csv'), args...)\n",
    "```\n",
    "\n",
    "Let's imagine a directive named `'include'`, which the client application is meant to interpret as: include the contents of the listed StarTable CSV files along with the contents of the file you're reading right now. Such a directive could look like:\n",
    "```\n",
    "***include\n",
    "bar.csv\n",
    "baz.csv\n",
    "```\n",
    "Note that there's nothing magical about the name \"include\"; it isn't StarTable syntax. This name, and how such a directive should be interpreted, is entirely defined by the application. We could just as easily imagine a `rename_tables` directive requiring certain table names to be amended upon reading. \n",
    "\n",
    "But let's stick to the \"include\" example for now. The application wants to interpret the `include` directive above as: read two additional files and throw all the read blocks together. Perhaps even recursively, i.e. similarly handle any `include` directives encountered in `bar.csv` and `baz.csv` and so on. A handler for this could be designed to be used as\n",
    "\n",
    "```\n",
    "block_gen = handle_includes(read_csv('foo.csv'), input_dir='./some_dir/', recursive=True)\n",
    "```\n",
    "\n",
    "and look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from pdtable import BlockGenerator\n",
    "\n",
    "def handle_includes(bg: BlockGenerator, input_dir, recursive: bool = False) -> BlockGenerator:\n",
    "    \"\"\"Handles 'include' directives, optionally recursively.\n",
    "\n",
    "    Handles 'include' directives.\n",
    "    'include' directives must contain a list of files located in directory 'input_dir'.\n",
    "\n",
    "    Optionally handles 'include' directives recursively. No check is done for circular references.\n",
    "    For example, if file1.csv includes file2.csv, and file2.csv includes file1.csv, then infinite\n",
    "    recursion ensues upon reading either file1.csv or file2.csv with 'recursive' set to True.\n",
    "\n",
    "    Args:\n",
    "        bg:\n",
    "            A block generator returned by read_csv\n",
    "\n",
    "        input_dir:\n",
    "            Path of directory in which include files are located.\n",
    "\n",
    "        recursive:\n",
    "            Handle 'include' directives recursively, i.e. 'include' directives in files themselves\n",
    "            read as a consequence of an 'include' directive, will be handled. Default is False.\n",
    "\n",
    "    Yields:\n",
    "        A block generator yielding blocks from...\n",
    "        * if recursive, the entire tree of files in 'include' directives.\n",
    "        * if not recursive, the top-level file and those files listed in its 'include' directive (if\n",
    "          any).\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    deep_handler = (\n",
    "        functools.partial(handle_includes, input_dir=input_dir, recursive=recursive)\n",
    "        if recursive\n",
    "        else lambda x: x\n",
    "    )\n",
    "\n",
    "    for block_type, block in bg:\n",
    "        if block_type == BlockType.DIRECTIVE:\n",
    "            directive: Directive = block\n",
    "            if directive.name == \"include\":\n",
    "                # Don't emit this directive block; handle it.\n",
    "                for filename in directive.lines:\n",
    "                    yield from deep_handler(read_csv(Path(input_dir) / filename))\n",
    "            else:\n",
    "                yield block_type, block\n",
    "        else:\n",
    "            yield block_type, block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is only an example implementation. You're welcome to poach it, but note that no check is done for circular references! E.g. if `bar.csv` contains an `include` directive pointing to `foo.csv` then calling `handle_includes` with `recursive=True` will result in a stack overflow. You may want to perform such a check if relevant for your application. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-",
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
